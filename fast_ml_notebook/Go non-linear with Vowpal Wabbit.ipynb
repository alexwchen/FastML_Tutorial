{
 "metadata": {
  "name": "Go non-linear with Vowpal Wabbit"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Go non-linear with Vowpal Wabbit\n",
      "See http://fastml.com/go-non-linear-with-vowpal-wabbit/ for description\n",
      "    \n",
      "auc.py - compute AUC for a validation set\n",
      "\n",
      "csv2vw.py - convert original CSV to VW format\n",
      "\n",
      "vw2sub.py - convert VW predictions to a submission format"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.1 Convert Files\n",
      "(.csv -> .vw) Convert train.csv & test.csv to train.vw & test.vw"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "csv2vw.py train.csv train.vw\n",
      "csv2vw.py test.csv test.vw"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### ACTION is what we are trying to predict, wether granting access or not. (train.csv)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "ACTION,RESOURCE,MGR_ID,ROLE_ROLLUP_1,ROLE_ROLLUP_2,ROLE_DEPTNAME,ROLE_TITLE,ROLE_FAMILY_DESC,ROLE_FAMILY,ROLE_CODE\n",
      "1,39353,85475,117961,118300,123472,117905,117906,290919,117908\n",
      "1,17183,1540,117961,118343,123125,118536,118536,308574,118539\n",
      "1,36724,14457,118219,118220,117884,117879,267952,19721,117880\n",
      "1,36135,5396,117961,118343,119993,118321,240983,290919,118322\n",
      "1,42680,5905,117929,117930,119569,119323,123932,19793,119325\n",
      "0,45333,14561,117951,117952,118008,118568,118568,19721,118570\n",
      "1,25993,17227,117961,118343,123476,118980,301534,118295,118982\n",
      ".\n",
      ".\n",
      "."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.2 Train Neural Networks\n",
      "#### vw -d train.vw --nn 10"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "0.298053   0.298053            3         3.0   1.0000   0.8642        9\n",
      "0.785963   1.273873            6         6.0  -1.0000   0.9499        9\n",
      "0.478965   0.110567           11        11.0   1.0000   0.9539        9\n",
      "0.246256   0.013548           22        22.0   1.0000   1.0000        9\n",
      "0.220025   0.193794           44        44.0   1.0000   0.7037        9\n",
      "0.249402   0.279462           87        87.0   1.0000   0.9472        9\n",
      "0.280361   0.311320          174       174.0   1.0000   0.8466        9\n",
      "0.290994   0.301627          348       348.0   1.0000   0.5541        9\n",
      "0.256690   0.222386          696       696.0   1.0000   0.9374        9\n",
      "0.248151   0.239613         1392      1392.0  -1.0000   0.9742        9\n",
      "0.233282   0.218413         2784      2784.0   1.0000   0.9257        9\n",
      "0.213578   0.193873         5568      5568.0  -1.0000   0.9931        9\n",
      "0.205618   0.197657        11135     11135.0  -1.0000   0.7838        9\n",
      "0.197001   0.188383        22269     22269.0   1.0000   0.8382        9\n",
      "\n",
      "finished run\n",
      "number of examples = 32769\n",
      "weighted example sum = 32769\n",
      "weighted label sum = 28975\n",
      "average loss = 0.193767\n",
      "best constant = 0.88422\n",
      "total feature number = 294921"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.2 Train Neural Networks & Get Accuracy\n",
      "#### vw -d train.vw -c -f model_nn --nn 10\n",
      "#### vw -t -d test.vw -i model_nn -p nn_out.txt\n",
      "#### python auc.py test.vw nn_out.txt \n",
      "\n",
      "--nn number: number means numbers of hidden layer"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "final_regressor = model_nn\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "0.298053   0.298053            3         3.0   1.0000   0.8642        9\n",
      "0.785963   1.273873            6         6.0  -1.0000   0.9499        9\n",
      "0.478345   0.109202           11        11.0   1.0000   0.9526        9\n",
      "0.245944   0.013543           22        22.0   1.0000   1.0000        9\n",
      "0.219715   0.193487           44        44.0   1.0000   0.7072        9\n",
      "0.249186   0.279341           87        87.0   1.0000   0.9449        9\n",
      "0.278906   0.308627          174       174.0   1.0000   0.7399        9\n",
      "0.285319   0.291732          348       348.0   1.0000   0.6418        9\n",
      "0.252158   0.218997          696       696.0   1.0000   0.9440        9\n",
      "0.244476   0.236793         1392      1392.0  -1.0000   0.9839        9\n",
      "0.231286   0.218097         2784      2784.0   1.0000   0.9516        9\n",
      "0.212076   0.192866         5568      5568.0  -1.0000   0.8727        9\n",
      "0.204526   0.196975        11135     11135.0  -1.0000   0.7309        9\n",
      "0.196411   0.188296        22269     22269.0   1.0000   0.8564        9\n",
      "\n",
      "finished run\n",
      "number of examples = 32769\n",
      "weighted example sum = 32769\n",
      "weighted label sum = 28975\n",
      "average loss = 0.193077\n",
      "best constant = 0.88422\n",
      "total feature number = 294921\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "only testing\n",
      "Num weight bits = 18\n",
      "learning rate = 10\n",
      "initial_t = 1\n",
      "power_t = 0.5\n",
      "predictions = nn_out.txt\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "1.745731   1.745731            3         3.0   3.0000   1.0000        9\n",
      "9.487297   17.228863           6         6.0   6.0000   0.9083        9\n",
      "35.590541  66.914435          11        11.0  11.0000   0.9747        9\n",
      "151.507630 267.424718         22        22.0  22.0000   0.9392        9\n",
      "625.705869 1099.904108         44        44.0  44.0000   0.9754        9\n",
      "2484.673574 4386.873087         87        87.0  87.0000   0.9121        9\n",
      "10014.844465 17545.015356        174       174.0 174.0000   0.9277        9\n",
      "40214.493898 70414.143330        348       348.0 348.0000   1.0000        9\n",
      "161162.495849 282110.497800        696       696.0 696.0000   0.9523        9\n",
      "645269.946110 1129377.396372       1392      1392.0 1392.0000   0.9789        9\n",
      "2582318.725480 4519367.504849       2784      2784.0 2784.0000   1.0000        9\n",
      "10331731.458465 18081144.191451       5568      5568.0 5568.0000   1.0000        9\n",
      "41324456.652424 72322749.068439      11135     11135.0 11135.0000   0.9498        9\n",
      "165292903.652644 289272484.876594      22269     22269.0 22269.0000   0.9985        9\n",
      "661161672.133119 1157052708.835639      44537     44537.0 44537.0000   0.9359        9\n",
      "\n",
      "finished run\n",
      "number of examples = 58921\n",
      "weighted example sum = 58921\n",
      "weighted label sum = 1.73587e+09\n",
      "average loss = 1.1572e+09\n",
      "best constant = 29461.5\n",
      "total feature number = 530289\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "AUC: 0.0119823489477\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.3 Train Neural Networks & Quadratic Feature\n",
      "#### vw -d train.vw -f model_nn -q nn \n",
      "#### vw -t -d test.vw -i model_nn -p nn_out.txt\n",
      "#### python auc.py test.vw nn_out.txt \n",
      "A basic scenario for one namespace called \u201cn\u201d. You could create quadratic features like this:\n",
      "    \n",
      "-q nn: nn means 2 sets name space?\n",
      "    \n",
      "vw -d data.vw -q nn\n",
      "\n",
      "Cubic features must involve three sets\n",
      "\n",
      "vw -d data.vw --cubic nnn\n",
      "\n",
      "Polynomial features can be combined with a neural network or used separately."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "creating quadratic features for pairs: nn \n",
      "final_regressor = model_nn\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "0.809189   0.809189            3         3.0   1.0000   0.1380        9\n",
      "0.844094   0.878998            6         6.0  -1.0000   0.3200        9\n",
      "0.600964   0.309208           11        11.0   1.0000   0.7573        9\n",
      "0.396161   0.191357           22        22.0   1.0000   0.9917        9\n",
      "0.336188   0.276216           44        44.0   1.0000   0.7488        9\n",
      "0.311189   0.285607           87        87.0   1.0000   0.9320        9\n",
      "0.315461   0.319733          174       174.0   1.0000   0.9761        9\n",
      "0.316913   0.318366          348       348.0   1.0000   0.6469        9\n",
      "0.278261   0.239608          696       696.0   1.0000   1.0000        9\n",
      "0.263897   0.249533         1392      1392.0  -1.0000   0.9833        9\n",
      "0.243463   0.223029         2784      2784.0   1.0000   0.9176        9\n",
      "0.221693   0.199923         5568      5568.0  -1.0000   1.0000        9\n",
      "0.209789   0.197883        11135     11135.0  -1.0000   0.5799        9\n",
      "0.196589   0.183388        22269     22269.0   1.0000   0.7693        9\n",
      "\n",
      "finished run\n",
      "number of examples = 32769\n",
      "weighted example sum = 32769\n",
      "weighted label sum = 28975\n",
      "average loss = 0.190776\n",
      "best constant = 0.88422\n",
      "total feature number = 294921\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "only testing\n",
      "Num weight bits = 18\n",
      "learning rate = 10\n",
      "initial_t = 1\n",
      "power_t = 0.5\n",
      "predictions = nn_out.txt\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "1.731965   1.731965            3         3.0   3.0000   1.0000        9\n",
      "9.199316   16.666667           6         6.0   6.0000   1.0000        9\n",
      "36.205211  68.612285          11        11.0  11.0000   0.8201        9\n",
      "153.488082 270.770953         22        22.0  22.0000   1.0000        9\n",
      "627.316622 1101.145161         44        44.0  44.0000   1.0000        9\n",
      "2484.192852 4384.252251         87        87.0  87.0000   0.8582        9\n",
      "10022.107676 17560.022500        174       174.0 174.0000   1.0000        9\n",
      "40236.696023 70451.284371        348       348.0 348.0000   1.0000        9\n",
      "161211.067693 282185.439363        696       696.0 696.0000   0.2419        9\n",
      "645334.280906 1129457.494118       1392      1392.0 1392.0000   0.7686        9\n",
      "2582458.249783 4519582.218660       2784      2784.0 2784.0000   0.6941        9\n",
      "10332042.478699 18081626.707615       5568      5568.0 5568.0000   0.6587        9\n",
      "41325102.286071 72323729.375606      11135     11135.0 11135.0000   1.0000        9\n",
      "165294269.191944 289274570.386204      22269     22269.0 22269.0000   1.0000        9\n",
      "661164299.796291 1157056598.679361      44537     44537.0 44537.0000   1.0000        9\n",
      "\n",
      "finished run\n",
      "number of examples = 58921\n",
      "weighted example sum = 58921\n",
      "weighted label sum = 1.73587e+09\n",
      "average loss = 1.15721e+09\n",
      "best constant = 29461.5\n",
      "total feature number = 530289\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "AUC: 0.0923794976239"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.4 Train Neural Networks & Cubic Feature\n",
      "#### vw -d train.vw -f model_nn --cubic nnn\n",
      "#### vw -t -d test.vw -i model_nn -p nn_out.txt\n",
      "#### python auc.py test.vw nn_out.txt "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "creating cubic features for triples: eee \n",
      "final_regressor = model_nn\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "0.881427   0.881427            3         3.0   1.0000   0.0840      521\n",
      "0.889423   0.897419            6         6.0  -1.0000   0.1990      521\n",
      "0.707428   0.489033           11        11.0   1.0000   0.4950      521\n",
      "0.511972   0.316515           22        22.0   1.0000   0.7145      521\n",
      "0.425960   0.339948           44        44.0   1.0000   0.6097      521\n",
      "0.371266   0.315300           87        87.0   1.0000   0.8276      521\n",
      "0.357895   0.344525          174       174.0   1.0000   0.8217      521\n",
      "0.342287   0.326679          348       348.0   1.0000   0.6899      521\n",
      "0.293832   0.245377          696       696.0   1.0000   0.9540      521\n",
      "0.276339   0.258847         1392      1392.0  -1.0000   0.9322      521\n",
      "0.257468   0.238596         2784      2784.0   1.0000   0.9581      521\n",
      "0.233323   0.209178         5568      5568.0  -1.0000   0.8323      521\n",
      "0.220930   0.208536        11135     11135.0  -1.0000   0.5727      521\n",
      "0.208206   0.195481        22269     22269.0   1.0000   0.9281      521\n",
      "\n",
      "finished run\n",
      "number of examples = 32769\n",
      "weighted example sum = 32769\n",
      "weighted label sum = 28975\n",
      "average loss = 0.202292\n",
      "best constant = 0.88422\n",
      "total feature number = 17072649\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "only testing\n",
      "Num weight bits = 18\n",
      "learning rate = 10\n",
      "initial_t = 1\n",
      "power_t = 0.5\n",
      "predictions = nn_out.txt\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "1.757669   1.757669            3         3.0   3.0000   1.0000      521\n",
      "9.212168   16.666667           6         6.0   6.0000   1.0000      521\n",
      "38.087486  72.737868          11        11.0  11.0000   0.3895      521\n",
      "154.327014 270.566543         22        22.0  22.0000   1.0000      521\n",
      "628.405120 1102.483226         44        44.0  44.0000   1.0000      521\n",
      "2488.196379 4391.238596         87        87.0  87.0000   0.5946      521\n",
      "10024.608822 17561.021265        174       174.0 174.0000   1.0000      521\n",
      "40242.308822 70460.008823        348       348.0 348.0000   0.8398      521\n",
      "161228.119479 282213.930136        696       696.0 696.0000   0.0271      521\n",
      "645358.281745 1129488.444010       1392      1392.0 1392.0000   0.6809      521\n",
      "2582536.454989 4519714.628233       2784      2784.0 2784.0000   0.9608      521\n",
      "10332165.964294 18081795.473599       5568      5568.0 5568.0000   0.5854      521\n",
      "41325361.311827 72324123.965870      11135     11135.0 11135.0000   0.8087      521\n",
      "165294728.769464 289275230.533501      22269     22269.0 22269.0000   1.0000      521\n",
      "661165258.804661 1157058057.141010      44537     44537.0 44537.0000   1.0000      521\n",
      "\n",
      "finished run\n",
      "number of examples = 58921\n",
      "weighted example sum = 58921\n",
      "weighted label sum = 1.73587e+09\n",
      "average loss = 1.15721e+09\n",
      "best constant = 29461.5\n",
      "total feature number = 30697841\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "AUC: 0.24149694501"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.5 Train on quadratic Features along\n",
      "\n",
      "vw -d train.vw -q nn\n",
      "\n",
      "vw -d data.vw --cubic nnn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.6 Train with Logistic Regression\n",
      "\n",
      "#### vw -d train.vw -k -c -f nn_model --loss_function logistic -b 25 --passes 20 -q ee --l2 0.0000005\n",
      "#### vw -t -d test.vw -i nn_model -p nn_out.txt\n",
      "#### python auc.py test.vw nn_out.txt \n",
      "    \n",
      "-b means modify number of bits to avoid feature collision\n",
      "\n",
      "By default it uses 18 bits, so that\u2019s about 262k possible features. If you have more than that, they will collide, meaning that the software won\u2019t be able to distinguish between some of them. Fortunately you can increase a number of bits used for hashing so that you can get millions of features.\n",
      "    \n",
      "--l2 means regularization"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "creating quadratic features for pairs: ee \n",
      "final_regressor = nn_model\n",
      "Num weight bits = 25\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using l2 regularization = 5e-07\n",
      "creating cache_file = train.vw.cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "0.671919   0.671919            3         3.0   1.0000   0.0467       73\n",
      "0.660256   0.648592            6         6.0  -1.0000   0.1420       73\n",
      "0.601267   0.530480           11        11.0   1.0000   0.7536       73\n",
      "0.506946   0.412625           22        22.0   1.0000   1.3973       73\n",
      "0.457603   0.408260           44        44.0   1.0000   1.0195       73\n",
      "0.404933   0.351038           87        87.0   1.0000   2.3258       73\n",
      "0.373747   0.342560          174       174.0   1.0000   2.7257       73\n",
      "0.338234   0.302722          348       348.0   1.0000   2.2520       73\n",
      "0.288575   0.238915          696       696.0   1.0000   4.4502       73\n",
      "0.265166   0.241758         1392      1392.0  -1.0000   3.2022       73\n",
      "0.238757   0.212347         2784      2784.0   1.0000   3.2824       73\n",
      "0.209851   0.180946         5568      5568.0  -1.0000   2.7910       73\n",
      "0.193789   0.177723        11135     11135.0  -1.0000   1.6563       73\n",
      "0.176430   0.159070        22269     22269.0   1.0000   1.9458       73\n",
      "0.150550   0.124668        44537     44537.0   1.0000   4.5169       73\n",
      "0.122340   0.094130        89073     89073.0   1.0000   3.0049       73\n",
      "0.103330   0.084320       178146    178146.0   1.0000   5.4075       73\n",
      "0.092257   0.081183       356291    356291.0   1.0000   4.0546       73\n",
      "\n",
      "finished run\n",
      "number of examples = 655380\n",
      "weighted example sum = 655380\n",
      "weighted label sum = 579500\n",
      "average loss = 0.086756\n",
      "best constant = 0.88422\n",
      "total feature number = 47842740\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "only testing\n",
      "Num weight bits = 25\n",
      "learning rate = 10\n",
      "initial_t = 1\n",
      "power_t = 0.5\n",
      "predictions = nn_out.txt\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "6.038768   6.038768            3         3.0   3.0000   5.6049       73\n",
      "3.722214   1.405660            6         6.0   6.0000   4.9049       73\n",
      "16.441781  31.705261          11        11.0  11.0000   3.4434       73\n",
      "102.803046 189.164311         22        22.0  22.0000   4.5268       73\n",
      "514.832594 926.862142         44        44.0  44.0000   5.2329       73\n",
      "2218.796558 3962.387590         87        87.0  87.0000   3.3728       73\n",
      "9529.910980 16841.025402        174       174.0 174.0000   4.5314       73\n",
      "39265.753083 69001.595187        348       348.0 348.0000   2.7874       73\n",
      "159341.483066 279417.213048        696       696.0 696.0000   1.8952       73\n",
      "641343.655954 1123345.828843       1392      1392.0 1392.0000   2.8715       73\n",
      "2574630.506901 4507917.357848       2784      2784.0 2784.0000   1.8809       73\n",
      "10316413.497255 18058196.487608       5568      5568.0 5568.0000   1.4407       73\n",
      "41293723.379319 72276597.714388      11135     11135.0 11135.0000   3.8632       73\n",
      "165231660.917541 289180729.939285      22269     22269.0 22269.0000   5.8100       73\n",
      "661038554.895317 1156867714.316508      44537     44537.0 44537.0000   3.0501       73\n",
      "\n",
      "finished run\n",
      "number of examples = 58921\n",
      "weighted example sum = 58921\n",
      "weighted label sum = 1.73587e+09\n",
      "average loss = 1.15704e+09\n",
      "best constant = 29461.5\n",
      "total feature number = 4301233\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "AUC: 0.0738968092329"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}